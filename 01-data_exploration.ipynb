{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JolileErHCaH"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQu1f-QmGexE"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import REGION, BUCKET, PROJECT, DELIM, RAW_DATA_COLS, RENAMED_COLS, LABEL_COL, STRING_COLS, FLOAT_COLS\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8e1gHCT4GlCH"
   },
   "source": [
    "# Cloud Setup\n",
    "This section is only required if running on cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJnHrsFHGlVQ"
   },
   "outputs": [],
   "source": [
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "otbgjhTeGn-f"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2O9fiCiOGrxu"
   },
   "source": [
    "# Data Profiling\n",
    "Data profiling is done to better understand the data, and to see if there are any invalid data (e.g. out of bounds data, unexpected data types). No data preprocessing should be done here; it should be done in tf.transform so as to have a consistent data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob('data/raw/raw_data_invoices_2015-2017.csv')\n",
    "# df = pd.concat([pd.read_csv(\n",
    "#     f, usecols=RAW_DATA_COLS, quoting=csv.QUOTE_ALL, sep=',', encoding='utf-16', dtype='str'\n",
    "#     ) for f in files], ignore_index=True)\n",
    "# df.columns = RENAMED_COLS\n",
    "# acc_code_freq = df.groupby('acc_code').size().rename('count').reset_index()\n",
    "# acc_codes_to_include = list(acc_code_freq[acc_code_freq['count'] >= 30].acc_code)\n",
    "# df = df[df['acc_code'].isin(acc_codes_to_include)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        sep='\\t',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        usecols=RAW_DATA_COLS\n",
    "    )\n",
    "    df.columns = RENAMED_COLS\n",
    "    df['amount'] = df['amount'] \\\n",
    "        .str.replace(',', '') \\\n",
    "        .apply(lambda num: '-' + num if num.find('(') != -1 else num) \\\n",
    "        .str.replace('\\(|\\)', '') \\\n",
    "        .astype('float')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = read_data('data/raw/raw_data_invoices_2015-2017_20181110.txt')\n",
    "eval_df = read_data('data/raw/raw_data_invoices_2018_20181110.txt')\n",
    "df = pd.concat([train_df, eval_df], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "acc_code_freq = train_df.groupby('acc_code').size().rename('count').reset_index()\n",
    "acc_codes_to_include = list(acc_code_freq[acc_code_freq['count'] >= 50].acc_code)\n",
    "\n",
    "train_df = train_df[train_df['acc_code'].isin(acc_codes_to_include)]\n",
    "# eval_df = eval_df[eval_df['acc_code'].isin(acc_codes_to_include)]\n",
    "\n",
    "# df['invoice_date'] = pd.to_datetime(df['invoice_date'])\n",
    "# train_df = df[df.invoice_date.dt.date.between('01/04/2015', '31/03/2016')] # fy2015-2016\n",
    "# eval_df = df[df.invoice_date.dt.date.between('01/04/2017', '31/03/2018')] # fy2017\n",
    "# train_df['invoice_date'] = train_df['invoice_date'].dt.date.astype('str')\n",
    "# eval_df['invoice_date'] = eval_df['invoice_date'].dt.date.astype('str')\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-y2ucs2jHPrX"
   },
   "outputs": [],
   "source": [
    "ProfileReport(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(train_df).to_file('img/train.html')\n",
    "ProfileReport(eval_df).to_file('img/eval.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmEgecH0HRgZ"
   },
   "source": [
    "# Split Data\n",
    "Example uses 80-10-10 split for train, eval and test - change if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM_SEED = 42\n",
    "# x = df.drop(LABEL_COL, axis=1)\n",
    "# y = df[[LABEL_COL]]\n",
    "# x_train, x_eval, y_train, y_eval = train_test_split(x, y, random_state=RANDOM_SEED, train_size=0.8, stratify=y)\n",
    "# x_eval, x_test, y_eval, y_test = train_test_split(x_eval, y_eval, random_state=RANDOM_SEED, train_size=0.5, stratify=y_eval)\n",
    "# train_df = pd.concat([x_train, y_train], axis=1)\n",
    "# eval_df = pd.concat([x_eval, y_eval], axis=1)\n",
    "# test_df = pd.concat([x_test, y_test], axis=1)\n",
    "\n",
    "# # reorder columns\n",
    "# train_df = train_df[RENAMED_COLS]\n",
    "# eval_df = eval_df[RENAMED_COLS]\n",
    "# test_df = test_df[RENAMED_COLS]\n",
    "\n",
    "# len(train_df), len(eval_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_dc6vYGGv0H"
   },
   "outputs": [],
   "source": [
    "test_df = eval_df[eval_df.business_unit.isin(['CCY', 'MND'])]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlsukCL7I9sg"
   },
   "outputs": [],
   "source": [
    "def export_datasets(on_cloud=False):\n",
    "    if on_cloud:\n",
    "        data_dir = 'gs://{bucket}/{project}/data/split'.format(bucket=BUCKET, project=PROJECT)\n",
    "    else:\n",
    "        data_dir = 'data/split'\n",
    "    \n",
    "    if not on_cloud:\n",
    "        if not os.path.exists('data'):\n",
    "            os.mkdir('data')\n",
    "        if not os.path.exists('data/split'):\n",
    "            os.mkdir('data/split')\n",
    "        \n",
    "#     def export_df(df, filename):\n",
    "#         full_path = os.path.join(data_dir, filename)\n",
    "#         csv_str = '\\n'.join(DELIM.join(str(r) for r in rec) for rec in df.to_records(index=False))\n",
    "#         with open(full_path, 'w') as f:\n",
    "#             f.write(csv_str)\n",
    "    \n",
    "#     export_df(train_df, 'train.csv')\n",
    "#     export_df(eval_df, 'eval.csv')\n",
    "#     export_df(test_df, 'test.csv')\n",
    "\n",
    "    def export_df(df, filename):\n",
    "        full_path = os.path.join(data_dir, filename)\n",
    "        df.to_csv(full_path, sep='\\t', quoting=csv.QUOTE_NONE, index=False)\n",
    "        \n",
    "    export_df(train_df, 'train.tsv')\n",
    "    export_df(eval_df, 'eval.tsv')\n",
    "    export_df(test_df, 'test.tsv')\n",
    "    \n",
    "    if not os.path.exists('data/misc'):\n",
    "        os.mkdir('data/misc')\n",
    "    with open('./data/misc/labels.txt', 'w') as f:\n",
    "        label_vocab = DELIM.join(list(df[LABEL_COL].astype('str').unique()))\n",
    "        f.write(label_vocab)\n",
    "  \n",
    "    return\n",
    "  \n",
    "export_datasets(on_cloud=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "01-data_preproc.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
