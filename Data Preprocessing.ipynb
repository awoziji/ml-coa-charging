{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 6\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_THRESHOLD = 20\n",
    "MIN_SAMPLES_PER_CLASS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_STOPWORDS = [\n",
    "    'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec',\n",
    "    'janurary', 'february', 'march', 'april', 'june', 'july', 'august', 'september', 'october', 'november', 'december'\n",
    "    'fy', 'mr', 'ms', 'mrs', 'pte', 'ltd'\n",
    "]\n",
    "for stopword in ADD_STOPWORDS:\n",
    "    nlp.vocab[stopword].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "\t'Fiscal Year (Accounting Date)': np.int64,\n",
    "\t'Business Unit': str,\n",
    "\t'Account Code': str,\n",
    "\t'Account Description': str,\n",
    "\t'Voucher ID': str,\n",
    "\t'Voucher Description': str,\n",
    "\t'Voucher Origin': str,\n",
    "\t'Vendor ID': str,\n",
    "\t'Vendor First Name': str,\n",
    "\t'Voucher Line Description': str,\n",
    "\t'Voucher Line Long Description': str,\n",
    "\t'Payment Voucher Line Amount S$ (Excluding GST, Including Freight S$)': str\n",
    "}\n",
    "df = pd.read_csv('data/raw/Raw Data for COA classification_einv1.csv', dtype=dtypes) \\\n",
    "    .append(pd.read_csv('data/raw/Raw Data for COA classification_einv2.csv', dtype=dtypes))\n",
    "df.columns = [\n",
    "    'fy',\n",
    "    'business_unit',\n",
    "    'acc_code',\n",
    "    'acc_descr',\n",
    "    'voucher_id',\n",
    "    'voucher_descr',\n",
    "    'voucher_origin',\n",
    "    'vendor_id',\n",
    "    'vendor_name',\n",
    "    'voucher_line_descr',\n",
    "    'voucher_line_long_descr',\n",
    "    'payment_voucher_amt'\n",
    "]\n",
    "df = df.assign(\n",
    "    fy = df.fy.astype('int64'),\n",
    "    business_unit = df.business_unit.astype('category'),\n",
    "    acc_code = df.acc_code.astype('object'),\n",
    "    acc_descr = df.acc_descr.astype('object'),\n",
    "    voucher_id = df.voucher_id.astype('object'),\n",
    "    voucher_descr = df.voucher_descr.astype('object'),\n",
    "    vendor_id = df.vendor_id.astype('category'),\n",
    "    vendor_name = df.vendor_name.astype('object'),\n",
    "    voucher_line_descr = df.voucher_line_descr.astype('object'),\n",
    "    voucher_line_long_descr = df.voucher_line_long_descr.astype('object'),\n",
    "    payment_voucher_amt = np.float64(df.payment_voucher_amt.str.replace(',', '').str.replace(r'\\(([\\d.]+)\\)', r'-\\1')),\n",
    "    voucher_full_descr = df.voucher_descr + ' ' + df.voucher_line_long_descr\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc_descrs, proc_vendor_names = [], []\n",
    "for doc in nlp.pipe(df.voucher_full_descr.str.lower().astype('unicode').values, batch_size=64, n_threads=-1, disable=['tagger', 'parser', 'ner']):\n",
    "    try:\n",
    "        proc_descrs.append(' '.join([word.text for word in doc if word.is_alpha and not word.is_stop and len(word.text) > 1]))\n",
    "    except Exception as e:\n",
    "        proc_descrs.append('')\n",
    "for doc in nlp.pipe(df.vendor_name.str.lower().astype('unicode').values, batch_size=64, n_threads=-1, disable=['tagger', 'parser', 'ner']):\n",
    "    try:\n",
    "        proc_vendor_names.append(' '.join([word.text for word in doc if word.is_alpha and not word.is_stop and len(word.text) > 1]))\n",
    "    except Exception as e:\n",
    "        proc_vendor_names.append('')\n",
    "df = df.assign(voucher_descr_proc = proc_descrs, vendor_name_proc = proc_vendor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 20):\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 20):\n",
    "    print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Infrequent Account Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = ['voucher_full_descr', 'voucher_descr_proc', 'vendor_name', 'vendor_name_proc', 'payment_voucher_amt', 'business_unit']\n",
    "y_var = ['acc_code']\n",
    "\n",
    "# remove infrequent acc codes globally\n",
    "freq_codes = df.groupby('acc_code').size().reset_index(name='freq')\n",
    "freq_codes = freq_codes[freq_codes.freq >= FREQ_THRESHOLD][['acc_code']]\n",
    "df = df.merge(freq_codes, how='inner', on='acc_code')\n",
    "\n",
    "df = df[x_vars + y_var + ['acc_descr']].dropna()\n",
    "acc_mapping = df[['acc_code', 'acc_descr']] \\\n",
    "    .drop_duplicates(subset=['acc_code', 'acc_descr']) \\\n",
    "    .reset_index(drop=True) \n",
    "acc_mapping = acc_mapping \\\n",
    "    .assign(acc_descr = acc_mapping.acc_code + ' - ' + acc_mapping.acc_descr) \\\n",
    "    .reset_index() \\\n",
    "    [['acc_code', 'acc_descr', 'index']]\n",
    "df = df[x_vars + y_var]\n",
    "acc_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test Set (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[x_vars]\n",
    "y = df[y_var]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.08, random_state=42, stratify=y)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "\n",
    "# check that stratification worked successfully\n",
    "assert sum(np.sort(y_train.acc_code.unique()) == np.sort(y_val.acc_code.unique())) == y_train.acc_code.unique().shape[0]\n",
    "assert sum(np.sort(y_train.acc_code.unique()) == np.sort(y_test.acc_code.unique())) == y_train.acc_code.unique().shape[0]\n",
    "\n",
    "print(\n",
    "    'x_train.shape = ', x_train.shape, '\\n',\n",
    "    'y_train.shape = ', y_train.shape, '\\n',\n",
    "    'x_val.shape = ', x_val.shape, '\\n',\n",
    "    'y_val.shape = ', y_val.shape, '\\n',\n",
    "    'x_test.shape = ', x_test.shape, '\\n',\n",
    "    'y_test.shape = ', y_test.shape,\n",
    "    sep = ''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Sample Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "for acc_code in y_train.acc_code.unique():\n",
    "    df_with_acc_code = train[train.acc_code == acc_code]\n",
    "    if len(df_with_acc_code) >= MIN_SAMPLES_PER_CLASS:\n",
    "        sample = df_with_acc_code\n",
    "    else:\n",
    "        sample = df_with_acc_code.sample(n=MIN_SAMPLES_PER_CLASS, replace=True, random_state=42)\n",
    "    result = result.append(sample, ignore_index=True)\n",
    "x_train = result[x_vars]\n",
    "y_train = result[y_var]\n",
    "y_train.groupby('acc_code').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather('data/processed/coa_einv.feather')\n",
    "x_train.reset_index(drop=True).to_feather('data/processed/train/x_train.feather')\n",
    "x_val.reset_index(drop=True).to_feather('data/processed/val/x_val.feather')\n",
    "x_test.reset_index(drop=True).to_feather('data/processed/test/x_test.feather')\n",
    "y_train.reset_index(drop=True).to_feather('data/processed/train/y_train.feather')\n",
    "y_val.reset_index(drop=True).to_feather('data/processed/val/y_val.feather')\n",
    "y_test.reset_index(drop=True).to_feather('data/processed/test/y_test.feather')\n",
    "acc_mapping.reset_index(drop=True).to_feather('data/misc/acc_mapping.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
