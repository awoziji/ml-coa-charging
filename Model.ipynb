{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from helpers import gen_embeddings_index, gen_acc_mappings, gen_tokenizer, convert_text_to_seq, get_labels, gen_embedding_weights\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 6\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCR_COL_NAME = 'voucher_descr_proc'\n",
    "VENDOR_COL_NAME = 'vendor_name_proc'\n",
    "MAX_DESCR_LENGTH = 30\n",
    "MAX_VENDOR_LENGTH = 8\n",
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 100\n",
    "N_CONV_FILTERS = 512\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_feather('data/processed/train/x_train.feather')\n",
    "y_train = pd.read_feather('data/processed/train/y_train.feather')\n",
    "x_val = pd.read_feather('data/processed/val/x_val.feather')\n",
    "y_val = pd.read_feather('data/processed/val/y_val.feather')\n",
    "x_test = pd.read_feather('data/processed/test/x_test.feather')\n",
    "y_test = pd.read_feather('data/processed/test/y_test.feather')\n",
    "acc_mapping_df = pd.read_feather('data/misc/acc_mapping.feather')\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train.voucher_descr_proc.str.count(' ') + 1).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Embeddings Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = gen_embeddings_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Mappings for Interpreting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_indices, acc_index_to_code, acc_index_to_descr = gen_acc_mappings(acc_mapping_df)\n",
    "acc_index_to_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('<--------Fitting tokenizer on texts-------->')\n",
    "tokenizer = gen_tokenizer(list(x_train[DESCR_COL_NAME].values) + list(x_train[VENDOR_COL_NAME].values), MAX_NUM_WORDS)\n",
    "print('<-------Converting text to sequences------->')\n",
    "descr_train = convert_text_to_seq(tokenizer, x_train[DESCR_COL_NAME].values, MAX_DESCR_LENGTH)\n",
    "descr_val = convert_text_to_seq(tokenizer, x_val[DESCR_COL_NAME].values, MAX_DESCR_LENGTH)\n",
    "descr_test = convert_text_to_seq(tokenizer, x_test[DESCR_COL_NAME].values, MAX_DESCR_LENGTH)\n",
    "vendor_name_train = convert_text_to_seq(tokenizer, x_train[VENDOR_COL_NAME].values, MAX_VENDOR_LENGTH)\n",
    "vendor_name_val = convert_text_to_seq(tokenizer, x_val[VENDOR_COL_NAME].values, MAX_VENDOR_LENGTH)\n",
    "vendor_name_test = convert_text_to_seq(tokenizer, x_test[VENDOR_COL_NAME].values, MAX_VENDOR_LENGTH)\n",
    "print('<-----Converting labels to categorical----->')\n",
    "labels_train = get_labels(y_train, acc_mapping_df)\n",
    "labels_val = get_labels(y_val, acc_mapping_df)\n",
    "labels_test = get_labels(y_test, acc_mapping_df)\n",
    "print('Train labels shape: {}\\nValidation labels shape: {}\\nTest labels shape: {}'.format(labels_train.shape, labels_val.shape, labels_test.shape))\n",
    "print('<-----------Getting word indices----------->')\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_weights = gen_embedding_weights(num_words, EMBEDDING_DIM, word_index, embeddings_index)\n",
    "embedding_weights[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(ngrams=[1,2,3,4], embedding_trainable=True, incl_voucher_amt=True, dropout_rate=0.5):\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        num_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_weights],\n",
    "        # input_length=MAX_DESCR_LENGTH,\n",
    "        trainable=True, \n",
    "        name='embedding'\n",
    "    )\n",
    "    descr_input = tf.keras.layers.Input(shape=(MAX_DESCR_LENGTH,), dtype='int32', name='descr_input')\n",
    "    embedded_descr = embedding_layer(descr_input)\n",
    "    vendor_input = tf.keras.layers.Input(shape=(MAX_VENDOR_LENGTH,), dtype='int32', name='vendor_input')\n",
    "    embedded_vendor = embedding_layer(vendor_input)\n",
    "\n",
    "    # add convolutions for ngrams\n",
    "    descr_pools, vendor_pools = [], []\n",
    "    for ngram in ngrams:\n",
    "        descr_conv = tf.keras.layers.Conv1D(N_CONV_FILTERS, ngram, activation='relu', name='{}gram_descr_conv'.format(ngram))(embedded_descr)\n",
    "        descr_pool = tf.keras.layers.MaxPool1D(MAX_DESCR_LENGTH - ngram + 1, name='{}gram_descr_pool'.format(ngram))(descr_conv)\n",
    "        vendor_conv = tf.keras.layers.Conv1D(N_CONV_FILTERS / 2, ngram, activation='relu', name='{}gram_vendor_conv'.format(ngram))(embedded_vendor)\n",
    "        vendor_pool = tf.keras.layers.MaxPool1D(MAX_VENDOR_LENGTH - ngram + 1, name='{}gram_vendor_pool'.format(ngram))(vendor_conv)\n",
    "        descr_pools.append(descr_pool)\n",
    "        vendor_pools.append(vendor_pool)\n",
    "    \n",
    "    # concatenate all ngram features, flatten and add dropout\n",
    "    descr_total_pool =  tf.keras.layers.Concatenate(name='all_ngrams_descr_pool')(descr_pools)\n",
    "    vendor_total_pool =  tf.keras.layers.Concatenate(name='all_ngrams_vendor_pool')(vendor_pools)\n",
    "    descr_total_pool_flattened = tf.keras.layers.Flatten(name='descr_flatten')(descr_total_pool)\n",
    "    vendor_total_pool_flattened = tf.keras.layers.Flatten(name='vendor_flatten')(vendor_total_pool)\n",
    "    total_pool_flattened = tf.keras.layers.Concatenate(name='total_pool_flattened')([descr_total_pool_flattened, vendor_total_pool_flattened])\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate, name='dropout')(total_pool_flattened)\n",
    "\n",
    "    if incl_voucher_amt:\n",
    "        voucher_amt_input = tf.keras.layers.Input(shape=(1,), dtype='float32', name='voucher_amt_input')\n",
    "        voucher_amt_normalised = tf.keras.layers.BatchNormalization(name='voucher_amt_normalised')(voucher_amt_input)\n",
    "        pool_with_amt = tf.keras.layers.Concatenate(name='pool_with_amt')([dropout, voucher_amt_normalised])\n",
    "        preds = tf.keras.layers.Dense(len(acc_indices), activation='softmax', name='output')(pool_with_amt)\n",
    "        model = tf.keras.models.Model([descr_input, vendor_input, voucher_amt_input], preds)\n",
    "    else:\n",
    "        preds = tf.keras.layers.Dense(len(acc_indices), activation='softmax', name='output')(dropout)\n",
    "        model = tf.keras.models.Model([descr_input, vendor_input], preds)\n",
    "        \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return incl_voucher_amt, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "incl_voucher_amt, model = gen_model(\n",
    "    ngrams=[1,2,3,4], \n",
    "    embedding_trainable=True, \n",
    "    incl_voucher_amt=True, \n",
    "    dropout_rate=0.5\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    start_time = str(datetime.now())[:16].replace(r':', '')\n",
    "    print('Started training at {}'.format(start_time))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        histogram_freq=1, \n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        log_dir='./logs/{}'.format(start_time),\n",
    "        # embeddings_freq=1,\n",
    "        # embeddings_layer_names=['sentence_embedding'],\n",
    "        # embeddings_metadata={'sentence_embedding': 'data/misc/sentence_embedding.tsv'}\n",
    "    )\n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "    \n",
    "    if incl_voucher_amt:\n",
    "        model.fit(\n",
    "            [descr_train, vendor_name_train, x_train.payment_voucher_amt], labels_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=2,\n",
    "            validation_data=([descr_val, vendor_name_val, x_val.payment_voucher_amt], labels_val),\n",
    "            callbacks = [tensorboard_callback, early_stop_callback]\n",
    "        )\n",
    "    else:\n",
    "        model.fit(\n",
    "            [descr_train, vendor_name_train], labels_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=2,\n",
    "            validation_data=([descr_val, vendor_name_val], labels_val),\n",
    "            callbacks = [tensorboard_callback, early_stop_callback]\n",
    "        )\n",
    "    model.save('model/model {}.h5'.format(start_time))\n",
    "    \n",
    "    return start_time, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    if incl_voucher_amt:\n",
    "        preds = model.predict([descr_val, vendor_name_val, x_val.payment_voucher_amt])\n",
    "    else:\n",
    "        preds = model.predict([descr_val, vendor_name_val])\n",
    "        \n",
    "    preds_df = pd.DataFrame({\n",
    "        'actual_cls': np.argmax(labels_val, axis=1),\n",
    "        'pred_cls': np.argmax(preds, axis=1)\n",
    "    })\n",
    "    \n",
    "    if incl_voucher_amt:\n",
    "         preds_df = preds_df.assign(payment_voucher_amt = x_val.payment_voucher_amt)\n",
    "            \n",
    "    preds_df = preds_df.assign(\n",
    "        voucher_full_descr = x_val.voucher_full_descr,\n",
    "        voucher_descr_proc = x_val.voucher_descr_proc,\n",
    "        vendor_name = x_val.vendor_name,\n",
    "        vendor_name_proc = x_val.vendor_name_proc,\n",
    "        confidence = np.max(preds, axis=1),\n",
    "        actual = preds_df.actual_cls.map(lambda cls: acc_index_to_descr[cls]),\n",
    "        pred = preds_df.pred_cls.map(lambda cls: acc_index_to_descr[cls])\n",
    "    )\n",
    "    \n",
    "    preds_df = preds_df.assign(\n",
    "        wrong = preds_df.actual != preds_df.pred\n",
    "    )[['voucher_full_descr', 'voucher_descr_proc', 'vendor_name', 'vendor_name_proc', 'payment_voucher_amt', 'actual', 'pred', 'wrong', 'confidence']]\n",
    "    \n",
    "    if not incl_voucher_amt:\n",
    "        preds_df = preds_df.drop('payment_voucher_amt', axis=1)\n",
    "        \n",
    "    preds_df.to_excel('data/output/validation-{}.xlsx'.format(start_time), index=False)\n",
    "    \n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = predict()\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
